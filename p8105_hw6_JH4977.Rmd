---
title: "p8105_hw6_JH4977"
author: "jh4977"
date: "2025-12-01"
output: github_document
---
# Q1

```{r}
library(tidyverse)
library(broom)
library(purrr)
library(forcats)
library(ggplot2)

homicide_raw <- read_csv("data/homicide-data.csv")

homicide_df <- homicide_raw %>% 
  mutate(
    city_state = str_c(city, ", ", state),
    solved = if_else(disposition == "Closed by arrest", 1, 0),
    victim_sex = if_else(victim_sex == "Unknown", NA, victim_sex),
    victim_sex = factor(victim_sex, levels = c("Female", "Male")),
    victim_age = if_else(victim_age == "Unknown", NA, victim_age),
    victim_age = as.numeric(victim_age)
  ) %>% 
  filter(victim_race %in% c("White", "Black")) %>% 
  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", 
                            "Kansas City, MO", "Tulsa, AL")) %>% 
  filter(!is.na(victim_age))

# for baltimore use glm
baltimore <- homicide_df %>% 
  filter(city_state == "Baltimore, MD")

baltimore_fit <- glm(
  solved ~ victim_age + victim_sex + victim_race,
  data   = baltimore,
  family = binomial()
)

# use broom to tidy the data and do CI and exp--OR
baltimore_or <- baltimore_fit %>% 
  tidy(conf.int = TRUE, exponentiate = TRUE) %>% 
  filter(term == "victim_sexMale") %>% 
  select(term, estimate, conf.low, conf.high)

baltimore_or

# for other cities
# use nest to gather the data in each group and unest after running glm
city_results <- homicide_df %>% 
  group_by(city_state) %>% 
  nest() %>% 
  mutate(
    fit = map(
      data,
      ~ glm(solved ~ victim_age + victim_sex + victim_race,
            data = .x, family = binomial())
    ),
    tidied = map(fit, ~ tidy(.x, conf.int = TRUE, exponentiate = TRUE))
  ) %>% 
  unnest(tidied) %>% 
  filter(term == "victim_sexMale") %>% 
  select(city_state, estimate, conf.low, conf.high)

city_results
# order and plot
city_results %>% 
  ungroup() %>%                        
  ggplot(aes(x = reorder(city_state, estimate), 
             y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0) +
  geom_hline(yintercept = 1, linetype = 2) +
  coord_flip() +
  labs(
    x = "City, State",
    y = "Adjusted OR of being solved (Male vs Female victims)",
    title = "Estimated odds ratios and 95% CIs by city"
  ) +
  theme_minimal()
```

## Comment
The cities are ordered by the adjusted odds ratio (OR) of a homicide being solved for male victims relative to female victims. ORs below 1 indicate that cases with male victims are less likely to be solved than those with female victims, after adjusting for victim age and race, whereas ORs above 1 indicate the opposite.

Overall, most cities have estimated ORs fairly close to 1 and many 95% confidence intervals cross 1, suggesting little clear evidence of a strong sex difference in case resolution in most locations. A few cities at the top of the plot, such as Albuquerque, Stockton and Fresno, have ORs above 1 with wide intervals, indicating a tendency toward higher clearance for male victims but with substantial uncertainty. In contrast, cities near the bottom, including New York and Baton Rouge, have ORs below 1, suggesting that homicides involving male victims may be somewhat less likely to be solved than those involving female victims. However, given the wide intervals in many cities, these apparent differences should be interpreted cautiously.

# Q2

```{r}
library(purrr)
library(p8105.datasets)
data("weather_df")
```

```{r}
# have a look
weather_fit <- lm(tmax ~ tmin + prcp, data = weather_df)
glance(weather_fit)
tidy(weather_fit)

boot_once <- function(df, i) {
  boot_df <- df %>% 
    slice_sample(n = nrow(df), replace = TRUE)  
  
  fit <- lm(tmax ~ tmin + prcp, data = boot_df)
  
  r_sq <- glance(fit)$r.squared
  
  coefs <- tidy(fit)
  beta1 <- coefs %>% filter(term == "tmin") %>% pull(estimate)
  beta2 <- coefs %>% filter(term == "prcp") %>% pull(estimate)
  
  tibble(
    replicate = i,
    r_sq = r_sq,
    beta1_beta2 = beta1 / beta2
  )
}

set.seed(1)

boot_results <- map_dfr(1:5000, ~ boot_once(weather_df, .x))

boot_results
```

```{r}
boot_results %>% 
  pivot_longer(
    cols = c(r_sq, beta1_beta2),
    names_to = "stat",
    values_to = "value"
  ) %>% 
  ggplot(aes(x = value)) +
  geom_histogram(bins = 30, color = "white") +
  facet_wrap(~ stat, scales = "free_x") +
  labs(
    x = "Bootstrap estimate",
    y = "Count",
    title = "Bootstrap distributions of R^2 and β1 / β2"
  ) +
  theme_minimal()

```


```{r}
boot_results %>% 
  summarise(
    r_sq_lower  = quantile(r_sq, 0.025),
    r_sq_upper  = quantile(r_sq, 0.975),
    ratio_lower = quantile(beta1_beta2, 0.025),
    ratio_upper = quantile(beta1_beta2, 0.975)
  )

```

## Comment
### Interpretation of bootstrap results

We fit the linear model
\[
\texttt{tmax} = \beta_0 + \beta_1 \,\texttt{tmin} + \beta_2 \,\texttt{prcp} + \varepsilon .
\]

Using 5000 bootstrap samples, we obtained the bootstrap distributions of the
coefficient of determination \(R^2\) and the ratio
\(\hat{\beta}_1 / \hat{\beta}_2\), where \(\hat{\beta}_1\) is the coefficient
for \texttt{tmin} and \(\hat{\beta}_2\) is the coefficient for \texttt{prcp}.

The bootstrap distribution of \(R^2\) is approximately symmetric and highly
concentrated. The 95\% bootstrap confidence interval for \(R^2\) is
roughly \((0.934,\; 0.947)\), indicating that the model explains a very large
and fairly stable proportion (about 94\%) of the variability in daily maximum
temperature.

In contrast, the bootstrap distribution of \(\hat{\beta}_1 / \hat{\beta}_2\)
is much more spread out and clearly left–skewed. The 95\% bootstrap confidence
interval for this ratio is extremely wide, from about
\((-280,\; -126)\). Although the ratio is consistently negative—suggesting that
the effects of minimum temperature and precipitation on \(\texttt{tmax}\) act in
opposite directions—the large magnitude and width of this interval show that
\(\hat{\beta}_1 / \hat{\beta}_2\) is estimated with substantial uncertainty and
is much less precisely determined than \(R^2\).

# Q3
```{r}
library(modelr)
birthweight <- read_csv("data/birthweight.csv")

bw_df <- birthweight %>% 
  mutate(
    babysex = factor(babysex, levels = c(1, 2), labels = c("male", "female")),
    frace   = factor(frace),
    mrace   = factor(mrace),
    malform = factor(malform, levels = c(0, 1), labels = c("absent", "present"))
  )
# make sure no missing
sum(is.na(bw_df))
```

```{r}
# model building
bwt_mod_main <- lm(
  bwt ~ gaweeks + blength + bhead + babysex + 
    ppbmi + smoken + mrace + parity + malform,
  data = bw_df
)

summary(bwt_mod_main)

bw_df %>% 
  add_predictions(bwt_mod_main) %>% 
  add_residuals(bwt_mod_main) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = 0.3) +
  geom_hline(yintercept = 0, linetype = 2) +
  labs(
    x = "Fitted birthweight",
    y = "Residuals",
    title = "Residuals vs fitted values for proposed birthweight model"
  ) +
  theme_minimal()

```
## Comment
I fit a multiple linear regression model for birthweight with gestational
age, birth length, head circumference, baby’s sex, maternal pre-pregnancy
BMI, smoking during pregnancy, maternal race, parity and malformations as
predictors. This set of variables is meant to capture gestational
development, fetal size and key maternal characteristics related to
birthweight. The model explains about 71% of the variation in birthweight
(\(R^2 \approx 0.71\)), and most main effects are statistically
significant in the expected directions.

The residuals–versus–fitted plot shows residuals roughly centered around
zero with no strong curvature, suggesting that a linear specification is
reasonable. There is some increase in variability at higher fitted
birthweights and a few outliers, but no severe violation of model
assumptions is apparent.

```{r}
bwt_mod_len_ga <- lm(bwt ~ blength + gaweeks, data = bw_df)
summary(bwt_mod_len_ga)

bwt_mod_interact <- lm(bwt ~ bhead * blength * babysex, data = bw_df)
summary(bwt_mod_interact)

set.seed(1)

cv_df <- crossv_mc(bw_df, n = 100) 

cv_df <- cv_df %>% 
  mutate(
    train = map(train, as_tibble),
    test  = map(test,  as_tibble),
    
    mod_main     = map(train, ~ lm(
      bwt ~ gaweeks + blength + bhead + babysex +
        ppbmi + smoken + mrace + parity + malform,
      data = .x)),
    
    mod_len_ga   = map(train, ~ lm(bwt ~ blength + gaweeks, data = .x)),
    
    mod_interact = map(train, ~ lm(bwt ~ bhead * blength * babysex, data = .x)),
    
    rmse_main     = map2_dbl(mod_main,     test, ~ rmse(.x, .y)),
    rmse_len_ga   = map2_dbl(mod_len_ga,   test, ~ rmse(.x, .y)),
    rmse_interact = map2_dbl(mod_interact, test, ~ rmse(.x, .y))
  )

cv_long <- cv_df %>% 
  select(starts_with("rmse_")) %>% 
  pivot_longer(
    cols = everything(),
    names_to = "model",
    values_to = "rmse"
  ) %>% 
  mutate(
    model = recode(model,
      rmse_main     = "Proposed model",
      rmse_len_ga   = "Length + gestational age",
      rmse_interact = "Head*length*sex (full interaction)"
    )
  )

cv_long %>% 
  ggplot(aes(x = model, y = rmse)) +
  geom_boxplot() +
  labs(
    x = NULL,
    y = "RMSE on test sets",
    title = "Cross-validated prediction error for three birthweight models"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 20, hjust = 1))

cv_long %>% 
  group_by(model) %>% 
  summarise(
    mean_rmse = mean(rmse),
    sd_rmse   = sd(rmse)
  )

```

## Comment
Using 100 Monte–Carlo cross-validation splits, the proposed model has the
smallest prediction error (mean RMSE ≈ 278), followed by the model with
head*length*sex interactions (mean RMSE ≈ 289). The simplest model that
uses only length and gestational age performs worst, with a much larger
mean RMSE (≈ 332) and higher variability. This suggests that adding
gestational, maternal and smoking covariates substantially improves
out-of-sample prediction of birthweight, while the more complicated
interaction model does not outperform the proposed model despite its
greater complexity.


